{"cells":[{"cell_type":"markdown","metadata":{"id":"t-Zemczcjvoo"},"source":["\n","\n","# Multi-class Classification\n","### Introduction\n","\n","In the second part of this assignment, you will implement, train and evaluate several single layer neural networks to predict the correct category of a previously unseen example.\n","Machine learning models can be trained to analyze various financial indicators such as company earnings, macroeconomic factors, and market trends to predict stock price movements. This can be used by investors to make informed decisions about buying or selling stocks. We will employ the stock price movements dataset for this task. The dataset contains 50 instances each, of upwards, downwards and sideways stock price movement of a specific stock. The goal is to predict the stock price movement on unseen examples using the 4 input features derived from company earnings and macroeconomic factors.\n"]},{"cell_type":"markdown","metadata":{"id":"c19O1PI59nV7"},"source":["Before starting, we first import NumPy and PyTorch libraries."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"uQRh-HN82lNg","executionInfo":{"status":"ok","timestamp":1715110163017,"user_tz":-120,"elapsed":5608,"user":{"displayName":"Lalitha Sivakumar","userId":"10038052353497455901"}}},"outputs":[],"source":["import pandas as pd\n","import torch\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"PehXi8gMAKPn"},"source":["### Only for Google Colab users\n","\n","If you are using Google Colab, please upload the notebook and the dataset file to DS405B folder on your Google Drive. Then, open the notebook file with Google Colab by right-clicking the *.ipynb file. The code in the following cell mounts your google drive directories in your google Colab environment."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ox0dyXiA3YJ4","outputId":"4f7754f0-f01a-431e-f581-7bfa47d00e0a","executionInfo":{"status":"ok","timestamp":1715110258145,"user_tz":-120,"elapsed":65271,"user":{"displayName":"Lalitha Sivakumar","userId":"10038052353497455901"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["## only for google colab users\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"oxwMkt9bhAeG"},"source":["### Load Dataset\n","\n","The code for reading the `stock_price.csv` file is already implemented for you in the following code cell. Furthermore, in the code below we map the textual categories (upwards, downwards and sideways) to numerical class labels (0, 1 or 2) and split of the dataset into training (80%) and test (20%) datasets. The input features are also normalised to have zero mean and unit standard deviation."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"uIwEs1HZH1JQ","executionInfo":{"status":"ok","timestamp":1715112342324,"user_tz":-120,"elapsed":658,"user":{"displayName":"Lalitha Sivakumar","userId":"10038052353497455901"}}},"outputs":[],"source":["df = pd.read_csv('/content/stock_price.csv', index_col=None, header=None) # change the path to your own path\n","df.columns = ['x1', 'x2', 'x3', 'x4', 'y']\n","\n","d = {'upwards': 1,\n","     'downwards': 2,\n","     'sideways': 0,\n","}\n","\n","df['y'] = df['y'].map(d)\n","\n","# Assign features and target\n","\n","X = torch.tensor(df[['x1', 'x2', 'x3', 'x4']].values, dtype=torch.float)\n","y = torch.tensor(df['y'].values, dtype=torch.int)\n","\n","# Shuffling & train/test split\n","\n","torch.manual_seed(123)\n","shuffle_idx = torch.randperm(y.size(0), dtype=torch.long)\n","\n","X, y = X[shuffle_idx], y[shuffle_idx]\n","\n","percent80 = int(shuffle_idx.size(0)*0.8)\n","\n","X_train, X_test = X[shuffle_idx[:percent80]], X[shuffle_idx[percent80:]]\n","y_train, y_test = y[shuffle_idx[:percent80]], y[shuffle_idx[percent80:]]\n","\n","# Normalize (mean zero, unit variance)\n","\n","mu, sigma = X_train.mean(dim=0), X_train.std(dim=0)\n","X_train = (X_train - mu) / sigma\n","X_test = (X_test - mu) / sigma"]},{"cell_type":"markdown","source":["**Task 1.Logistic Regression for Multiclass Classification**"],"metadata":{"id":"GJuQ6kn5DmLy"}},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.optim as optim\n","\n","input_size = X.shape[1]\n","output_size = len(torch.unique(y))\n","\n","class LogisticRegression(nn.Module):\n","    def __init__(self, input_size, output_size):\n","        super(LogisticRegression, self).__init__()\n","        self.linear = nn.Linear(input_size, output_size)\n","\n","    def forward(self, x):\n","        logits = self.linear(x)\n","        return torch.sigmoid(logits)\n","\n","logistic_model = LogisticRegression(input_size, output_size)\n","\n","criterion = nn.CrossEntropyLoss() # for multi class classification\n","optimizer = optim.Adam(logistic_model.parameters(), lr=0.01) # Stochastic Gradient Descent\n","\n","# Training loop\n","num_epochs = 1000\n","batch_size = 50\n","for epoch in range(num_epochs):\n","    for i in range(0, len(X_train), batch_size):\n","        batch_X = X_train[i:i+batch_size]\n","        batch_y = y_train[i:i+batch_size]\n","\n","        outputs = logistic_model(batch_X)  #fw pass\n","        loss = criterion(outputs, batch_y.long())\n","\n","        optimizer.zero_grad() #zero gradients after loss\n","        loss.backward() #bw\n","        optimizer.step()\n","\n","with torch.no_grad():\n","    logistic_model.eval()\n","    outputs = logistic_model(X_test)\n","    _, predicted = torch.max(outputs, 1)\n","    correct = (predicted == y_test).sum().item()\n","    total = y_test.size(0)\n","    accuracy = correct / total\n","    print(f'Accuracy on test set: {accuracy:.2%}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HH54CmMkDsRr","executionInfo":{"status":"ok","timestamp":1715116662066,"user_tz":-120,"elapsed":2466,"user":{"displayName":"Lalitha Sivakumar","userId":"10038052353497455901"}},"outputId":"b5dc0c40-5743-4705-fdac-dc09e4b5f637"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy on test set: 90.00%\n"]}]},{"cell_type":"markdown","source":["**Task 2.Softmax Regression with custom implementation of Cross Entropy\n","Loss**"],"metadata":{"id":"OycCiv5Pp2mR"}},{"cell_type":"markdown","source":[],"metadata":{"id":"Ge_N5eByn0HE"}},{"cell_type":"code","source":["class SoftmaxRegression(nn.Module):\n","    def __init__(self, input_size, num_classes):\n","        super(SoftmaxRegression, self).__init__()\n","        self.fc = nn.Linear(input_size, num_classes)\n","    def forward(self, x):\n","        x = self.fc(x)\n","        return x\n","\n","def one_hot_encoding(labels, num_classes):\n","    one_hot = torch.zeros(labels.size(0), num_classes) #one hot tensor first fill with zeroes\n","    for i in range(len(labels)):  #in labels\n","        one_hot[i, labels[i]] = 1  # replace true key with 1\n","    return one_hot\n","\n","def softmax(logits):\n","    exp_logits = torch.exp(logits) # logit exp for softmax\n","    return exp_logits / torch.sum(exp_logits, dim=1, keepdim=True) #probability calculation\n","\n","def cross_entropy_loss(outputs, targets):\n","    log_probs = torch.log(outputs)  #log of softmax prob\n","    return -torch.mean(torch.sum(targets * log_probs, dim=1)) # compute loss\n","\n","#params\n","input_size = X_train.shape[1]\n","num_classes = len(df['y'].unique())\n","learning_rate = 0.07\n","num_epochs = 30\n","\n","model = SoftmaxRegression(input_size, num_classes)\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    logits = model(X_train) # fw pass\n","    outputs = softmax(logits) # find probability\n","    targets = one_hot_encoding(y_train, num_classes) # find loss\n","    loss = cross_entropy_loss(outputs, targets)\n","\n","    optimizer.zero_grad() #zero gradients after loss\n","    loss.backward() #bw pass\n","    optimizer.step()\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","\n","\n","with torch.no_grad():\n","    logits = model(X_test)\n","    outputs = softmax(logits)\n","    _, predicted = torch.max(outputs, 1)\n","    accuracy = (predicted == y_test).sum().item() / len(y_test)\n","    print(f'Accuracy: {accuracy * 100:.2f}%')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dvjKIojan0TR","executionInfo":{"status":"ok","timestamp":1715096471977,"user_tz":-120,"elapsed":197,"user":{"displayName":"Lalitha Sivakumar","userId":"10038052353497455901"}},"outputId":"37a80a9d-0a79-4d62-ddc6-87c13ccf15bb"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/30], Loss: 1.0931\n","Epoch [2/30], Loss: 0.9104\n","Epoch [3/30], Loss: 0.7713\n","Epoch [4/30], Loss: 0.6709\n","Epoch [5/30], Loss: 0.5978\n","Epoch [6/30], Loss: 0.5415\n","Epoch [7/30], Loss: 0.4962\n","Epoch [8/30], Loss: 0.4588\n","Epoch [9/30], Loss: 0.4276\n","Epoch [10/30], Loss: 0.4016\n","Epoch [11/30], Loss: 0.3799\n","Epoch [12/30], Loss: 0.3616\n","Epoch [13/30], Loss: 0.3459\n","Epoch [14/30], Loss: 0.3323\n","Epoch [15/30], Loss: 0.3201\n","Epoch [16/30], Loss: 0.3091\n","Epoch [17/30], Loss: 0.2989\n","Epoch [18/30], Loss: 0.2895\n","Epoch [19/30], Loss: 0.2806\n","Epoch [20/30], Loss: 0.2723\n","Epoch [21/30], Loss: 0.2644\n","Epoch [22/30], Loss: 0.2568\n","Epoch [23/30], Loss: 0.2495\n","Epoch [24/30], Loss: 0.2426\n","Epoch [25/30], Loss: 0.2360\n","Epoch [26/30], Loss: 0.2298\n","Epoch [27/30], Loss: 0.2240\n","Epoch [28/30], Loss: 0.2186\n","Epoch [29/30], Loss: 0.2135\n","Epoch [30/30], Loss: 0.2088\n","Accuracy: 96.67%\n"]}]},{"cell_type":"markdown","source":["**Task 3.Softmax Regression with torch.nn.functional.nll_loss**"],"metadata":{"id":"xePYbc5ww7_M"}},{"cell_type":"code","source":["import torch.nn.functional as F\n","model = SoftmaxRegression(input_size, num_classes)\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    logits = model(X_train)  # compute predicted logits\n","    outputs = F.log_softmax(logits, dim=1)  # apply log(softmax) to logits\n","    loss = F.nll_loss(outputs, y_train)  # nll\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()  #update\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')  # Print the loss for each epoch\n","\n","with torch.no_grad():\n","    logits = model(X_test)\n","    outputs = F.log_softmax(logits, dim=1)\n","    _, predicted = torch.max(outputs, 1)\n","    accuracy = (predicted == y_test).sum().item() / len(y_test)\n","    print(f'Accuracy: {accuracy * 100:.2f}%')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PUg_kIKfw8RY","executionInfo":{"status":"ok","timestamp":1715096583191,"user_tz":-120,"elapsed":188,"user":{"displayName":"Lalitha Sivakumar","userId":"10038052353497455901"}},"outputId":"3447989d-3687-432d-c55a-19b14b132b83"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/30], Loss: 1.8783\n","Epoch [2/30], Loss: 1.5530\n","Epoch [3/30], Loss: 1.2752\n","Epoch [4/30], Loss: 1.0521\n","Epoch [5/30], Loss: 0.8822\n","Epoch [6/30], Loss: 0.7556\n","Epoch [7/30], Loss: 0.6600\n","Epoch [8/30], Loss: 0.5863\n","Epoch [9/30], Loss: 0.5290\n","Epoch [10/30], Loss: 0.4845\n","Epoch [11/30], Loss: 0.4504\n","Epoch [12/30], Loss: 0.4240\n","Epoch [13/30], Loss: 0.4034\n","Epoch [14/30], Loss: 0.3868\n","Epoch [15/30], Loss: 0.3731\n","Epoch [16/30], Loss: 0.3614\n","Epoch [17/30], Loss: 0.3513\n","Epoch [18/30], Loss: 0.3423\n","Epoch [19/30], Loss: 0.3343\n","Epoch [20/30], Loss: 0.3270\n","Epoch [21/30], Loss: 0.3204\n","Epoch [22/30], Loss: 0.3142\n","Epoch [23/30], Loss: 0.3085\n","Epoch [24/30], Loss: 0.3029\n","Epoch [25/30], Loss: 0.2975\n","Epoch [26/30], Loss: 0.2921\n","Epoch [27/30], Loss: 0.2866\n","Epoch [28/30], Loss: 0.2812\n","Epoch [29/30], Loss: 0.2757\n","Epoch [30/30], Loss: 0.2702\n","Accuracy: 86.67%\n"]}]},{"cell_type":"markdown","source":["**Task 4.Softmax Regression with torch.nn.functional.cross_entropy**"],"metadata":{"id":"IfLZMGXGyEXn"}},{"cell_type":"code","source":["for epoch in range(num_epochs):\n","    logits = model(X_train)  # compute predicted logits\n","    outputs = F.log_softmax(logits, dim=1)\n","    loss = F.cross_entropy(logits, y_train)  #logits as input\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()  #update\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","\n","    with torch.no_grad():\n","      logits = model(X_test)\n","      outputs = F.log_softmax(logits, dim=1)\n","      _, predicted = torch.max(outputs, 1)\n","      accuracy = (predicted == y_test).sum().item() / len(y_test)\n","      print(f'Accuracy: {accuracy * 100:.2f}%')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oilyazwqyKg0","executionInfo":{"status":"ok","timestamp":1715096502586,"user_tz":-120,"elapsed":196,"user":{"displayName":"Lalitha Sivakumar","userId":"10038052353497455901"}},"outputId":"16e71ac6-be69-4e74-976c-b91f253db460"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/30], Loss: 0.2811\n","Accuracy: 83.33%\n","Epoch [2/30], Loss: 0.2755\n","Accuracy: 83.33%\n","Epoch [3/30], Loss: 0.2701\n","Accuracy: 86.67%\n","Epoch [4/30], Loss: 0.2648\n","Accuracy: 93.33%\n","Epoch [5/30], Loss: 0.2595\n","Accuracy: 93.33%\n","Epoch [6/30], Loss: 0.2543\n","Accuracy: 90.00%\n","Epoch [7/30], Loss: 0.2492\n","Accuracy: 90.00%\n","Epoch [8/30], Loss: 0.2442\n","Accuracy: 90.00%\n","Epoch [9/30], Loss: 0.2395\n","Accuracy: 90.00%\n","Epoch [10/30], Loss: 0.2349\n","Accuracy: 90.00%\n","Epoch [11/30], Loss: 0.2307\n","Accuracy: 93.33%\n","Epoch [12/30], Loss: 0.2267\n","Accuracy: 93.33%\n","Epoch [13/30], Loss: 0.2230\n","Accuracy: 96.67%\n","Epoch [14/30], Loss: 0.2196\n","Accuracy: 96.67%\n","Epoch [15/30], Loss: 0.2165\n","Accuracy: 96.67%\n","Epoch [16/30], Loss: 0.2135\n","Accuracy: 96.67%\n","Epoch [17/30], Loss: 0.2107\n","Accuracy: 96.67%\n","Epoch [18/30], Loss: 0.2080\n","Accuracy: 96.67%\n","Epoch [19/30], Loss: 0.2055\n","Accuracy: 96.67%\n","Epoch [20/30], Loss: 0.2031\n","Accuracy: 96.67%\n","Epoch [21/30], Loss: 0.2008\n","Accuracy: 96.67%\n","Epoch [22/30], Loss: 0.1986\n","Accuracy: 96.67%\n","Epoch [23/30], Loss: 0.1966\n","Accuracy: 96.67%\n","Epoch [24/30], Loss: 0.1946\n","Accuracy: 96.67%\n","Epoch [25/30], Loss: 0.1927\n","Accuracy: 96.67%\n","Epoch [26/30], Loss: 0.1909\n","Accuracy: 96.67%\n","Epoch [27/30], Loss: 0.1891\n","Accuracy: 96.67%\n","Epoch [28/30], Loss: 0.1874\n","Accuracy: 96.67%\n","Epoch [29/30], Loss: 0.1857\n","Accuracy: 96.67%\n","Epoch [30/30], Loss: 0.1840\n","Accuracy: 96.67%\n"]}]},{"cell_type":"markdown","source":["**Task 5.Softmax Regression with Mean Squared Error Loss**"],"metadata":{"id":"TYuDdgecuuth"}},{"cell_type":"code","source":["class MulticlassClassifier(nn.Module):\n","    def __init__(self, input_size, num_classes):\n","        super(MulticlassClassifier, self).__init__()\n","        self.fc1 = nn.Linear(input_size, num_classes)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        return x\n","\n","input_size = 4\n","num_classes = 3\n","model = MulticlassClassifier(input_size, num_classes)\n","optimizer = optim.Adam(model.parameters(), lr=0.06)\n","\n","num_epochs = 20\n","for epoch in range(num_epochs):\n","    outputs = model(X_train)\n","    # Apply softmax activation\n","    probs = F.softmax(outputs, dim=1)\n","    y_train_onehot = F.one_hot(y_train, num_classes).float() # i do one hot encoding for mse loss\n","    loss = F.mse_loss(probs, y_train_onehot) #mse loss\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","\n","with torch.no_grad():\n","    outputs = model(X_test)\n","    probs = F.softmax(outputs, dim=1)\n","    _, predicted = torch.max(probs, 1)\n","    correct = (predicted == y_test).sum().item()\n","    total = y_test.size(0)\n","    accuracy = correct / total\n","    print(f'Accuracy: {accuracy * 100:.2f}%')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ps1M06bpuZU0","executionInfo":{"status":"ok","timestamp":1715096924875,"user_tz":-120,"elapsed":377,"user":{"displayName":"Lalitha Sivakumar","userId":"10038052353497455901"}},"outputId":"9bc88c61-0593-4f25-c8f7-fcdec57bd1da"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/20], Loss: 0.2144\n","Epoch [2/20], Loss: 0.1806\n","Epoch [3/20], Loss: 0.1496\n","Epoch [4/20], Loss: 0.1237\n","Epoch [5/20], Loss: 0.1053\n","Epoch [6/20], Loss: 0.0942\n","Epoch [7/20], Loss: 0.0883\n","Epoch [8/20], Loss: 0.0851\n","Epoch [9/20], Loss: 0.0830\n","Epoch [10/20], Loss: 0.0813\n","Epoch [11/20], Loss: 0.0796\n","Epoch [12/20], Loss: 0.0779\n","Epoch [13/20], Loss: 0.0762\n","Epoch [14/20], Loss: 0.0746\n","Epoch [15/20], Loss: 0.0731\n","Epoch [16/20], Loss: 0.0716\n","Epoch [17/20], Loss: 0.0703\n","Epoch [18/20], Loss: 0.0691\n","Epoch [19/20], Loss: 0.0679\n","Epoch [20/20], Loss: 0.0668\n","Accuracy: 76.67%\n"]}]},{"cell_type":"markdown","source":["**Task 6.Linear Regression with Mean Squared Error Loss**"],"metadata":{"id":"PVHTo4hjrIiL"}},{"cell_type":"code","source":["class LinearRegression(nn.Module):\n","    def __init__(self, input_size, output_size):\n","        super(LinearRegression, self).__init__()\n","        self.linear = nn.Linear(input_size, output_size)\n","\n","    def forward(self, x):\n","        return self.linear(x)\n","\n","# input and output sizes\n","input_size = X_train.shape[1]\n","output_size = 1  #regressing to a continuous value\n","\n","model = LinearRegression(input_size, output_size)\n","criterion = nn.MSELoss()\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.07)\n","\n","num_epochs = 20\n","for epoch in range(num_epochs):\n","    outputs = model(X_train) #fw pass\n","    loss = criterion(outputs, y_train.float())\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","with torch.no_grad():\n","    model.eval()\n","    outputs = model(X_test)\n","    predicted = torch.round(outputs)\n","    correct = (predicted == y_test.float()).sum().item()\n","    total = y_test.size(0)\n","    accuracy = correct / total\n","    print(f'Accuracy on test set: {accuracy:.2%}')  #accuracy for this model is low, i am not sure why this is the case.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uEbM2GaZrIxB","executionInfo":{"status":"ok","timestamp":1715118622615,"user_tz":-120,"elapsed":826,"user":{"displayName":"Lalitha Sivakumar","userId":"10038052353497455901"}},"outputId":"74265ba1-f193-4864-ba5f-1e1d10a3fc1a"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy on test set: 30.00%\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}